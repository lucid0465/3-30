{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf1f2fe7",
   "metadata": {
    "id": "cf1f2fe7"
   },
   "source": [
    "# 주피터노트북"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed4c00",
   "metadata": {
    "id": "eaed4c00"
   },
   "outputs": [],
   "source": [
    "[현 디렉토리 찾기]\n",
    "import os\n",
    "os.getcwd() ## 현재 디렉토리\n",
    "os.listdir() ## dir\n",
    "\n",
    "[라이브러리 설치]\n",
    "주피터노트북에서\n",
    "!pip install scikit-learn\n",
    "!pip install mlxtend\n",
    "\n",
    "윈도우에서\n",
    "pip install scikit-learn\n",
    "\n",
    "[wrrning 제거]\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b8c9d3",
   "metadata": {
    "id": "e0b8c9d3"
   },
   "source": [
    "# 시험 주의 사항"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2d935",
   "metadata": {
    "id": "5fb2d935"
   },
   "source": [
    "# Numpy 기초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5895627",
   "metadata": {
    "id": "a5895627"
   },
   "outputs": [],
   "source": [
    "arr = np.array([0, 1, 2, 3,'A','B'])\n",
    "\n",
    "arr = np.array([[1, 2, 3], [10, 9, 8]])\n",
    "arr.min() # 1\n",
    "arr.max() # 10\n",
    "arr.mean() # 5.5\n",
    "arr.argmax() # 3, 최대값의 위치\n",
    "arr.argmin() # 0, 최소값의 위치\n",
    "    \n",
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "arr.ndim # 2\n",
    "arr.shape # (2,3)\n",
    "arr.dtype # dtype('int32')\n",
    "\n",
    "    \n",
    "arr = np.arange(6) # array([0, 1, 2, 3, 4, 5])\n",
    "arr.reshape(2, 3)\n",
    "## array([[0, 1, 2],\n",
    "##        [3, 4, 5]])\n",
    "\n",
    "[tolist]\n",
    "arr = np.arange(6) # array([0, 1, 2, 3, 4, 5])\n",
    "arr.tolist() # [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "[np.where]\n",
    "np.where(arr < 2, 1, 0) # 2보다 크면 1, 아니면 0 \n",
    "df['age_over'] = np.where(df['aa'] < 2, 1, 0) # 2보다 크면 1, 아니면 0 \n",
    "\n",
    "[log계산]\n",
    "np.log(10000)  #밑이 e인 자연로그\n",
    "np.log2(4) #밑이 2인 로그\n",
    "np.log10(10000)  #밑이 10인 상용로그"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7abc2",
   "metadata": {
    "id": "4ac7abc2"
   },
   "source": [
    "## Pandas 기초\n",
    "* 파일불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526e1e7",
   "metadata": {
    "id": "5526e1e7"
   },
   "outputs": [],
   "source": [
    "[데이터 읽어오기(csv,txt,xlsx,UTF-8 인코딩)]\n",
    "df = pd.read_csv('file.csv')\n",
    "txt: 텍스트 파일. 특정 구분자로 구분되어있다면 sep 인자에 명시(탭(tab)은 sep='\\t', '|' 는 sep='|'\n",
    "df = pd.read_csv('file.csv', encoding='cp949') # 한글파일 인코딩 에러시\n",
    "df = pd.read_csv('3.csv', sep='\\t') # 구분자: Tab일때\n",
    "df = pd.read_csv('3.csv', dtype = ({'jumin7': 'str'})) #불러올때 특정 컬럼의 type을 변경 할 수 있음\n",
    "                                        \n",
    "xlsx: 엑셀 파일. 특정 시트(sheet)의 데이터만 읽어올 경우 sheet_name 인자에 시트명 또는 순번을 입력해준다.\n",
    "df = pd.read_excel('file.xlsx')\n",
    "df = pd.read_excel('file.xlsx', sheet_name = 0)\n",
    "df = pd.read_excel('file.xlsx', sheet_name = 'first_sheet')\n",
    "                                        \n",
    "df.head(): 첫 5개 row 를 출력하며, 숫자 지정으로 개수를 조정할 수 있다.\n",
    "df.tail(): 마지막 5개 row 를 출력하며, 숫자 지정으로 개수를 조정할 수 있다.                                        \n",
    "df.len(): 기본 함수이지만 데이터 프레임에 사용하였을 때는 row 개수를 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95feebab",
   "metadata": {
    "id": "95feebab"
   },
   "source": [
    "* 데이터 프레임 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9b4f6",
   "metadata": {
    "id": "e7c9b4f6"
   },
   "outputs": [],
   "source": [
    "[DataFrame 생성]\n",
    "df = pd.DataFrame({'carat': [1,11],  'depth': [60,61]},index=['a','b'])\n",
    "df = pd.DataFrame({'k': [1,2,3,4,5], 'acc': [10,20,30,40,60]})\n",
    "\n",
    "[DataFrame 생성_머신러닝 Test값 입력용]\n",
    "df = df[0:1] # 0행만 슬라이싱해서 DF 생성\n",
    "dic = df.to_dict()  # 값 입력하기 쉽게 딕셔너리로 만들고 \n",
    "df = pd.DataFrame(dic)  # 딕셔너리로 DF만듬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed18c80",
   "metadata": {
    "id": "6ed18c80"
   },
   "source": [
    "* 기초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207ee22",
   "metadata": {
    "id": "5207ee22"
   },
   "outputs": [],
   "source": [
    "[함수 및 메서드]\n",
    "df['cut'].min(): 최소값\n",
    "df['cut'].max(): 최대값\n",
    "df['cut'].mean(): 평균값\n",
    "df['cut'].var(): 분산\n",
    "df['cut'].std(): 표준편차\n",
    "df['cut'].skew(): 왜도\n",
    "df['cut'].kurt(): 첨도\n",
    "df['cut'].idxmin(): 최소값의 위치(index)\n",
    "df['cut'].idxmax(): 최대값의 위치(index)\n",
    "df['cut'].count(): 개체수\n",
    "df['cut'].quantile(q = 0.25) : 1사분위수\n",
    "\n",
    "[속성(attribute)]\n",
    "df.ndim: 객체의 차원\n",
    "df.shape: 각 차원의 길이\n",
    "df.dtypes: 데이터 타입 #dtype와 구별, 복수형이라야 함\n",
    "df.columns: DF 컬럼명\n",
    "pd.Series(df.columns)  # columns에 0,1,2 index붙이기\n",
    "df.info(): DF 데이터타입, 결측치등 정보    \n",
    "df.describe(): 브라이틱스 stastics summary\n",
    "\n",
    "[astype]\n",
    "df.astype(str) # df전부 변경\n",
    "df.astype({'col1': 'int32', 'col2': 'int64'}) # 특정 걸럼 변경\n",
    "df['jumin7'].astype(str)\n",
    "df['jumin7'].astype({'jumin7':str}) #int를 str로 변경, str이라야 자를수 있음 \n",
    "df = pd.read_csv('3.csv', dtype = ({'jumin7': 'str'})) #불러올때 특정 컬럼의 type을 변경 할 수 있음\n",
    "\n",
    "[value_counts]\n",
    "df['Reviewer_Location'].value_counts()  # value_counts(dropna = True), dropna = True가 Default\n",
    "df['Reviewer_Location'].value_counts(dropna = False)  # dropna = False로 계산\n",
    "\n",
    "[nunique]\n",
    "df = df['Station_ID'].nunique().reset_index() #컬럼내 몇개의 고유값이 있는지 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12aede",
   "metadata": {
    "id": "5d12aede"
   },
   "source": [
    "* 데이타 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640dd23a",
   "metadata": {
    "id": "640dd23a"
   },
   "outputs": [],
   "source": [
    "[결측치]\n",
    "df.isna(), df.isnull()\n",
    "df.notna(), df.notnull()\n",
    "df.isna().sum() # 각 변수별 결측치 갯수\n",
    "df.isna().sum(axis=1) # 각 row별 결측치 갯수\n",
    "\n",
    "df.dropna()\n",
    "df.dropna(how='any')\n",
    "df.dropna(how='all')\n",
    "df.dropna(subset = ['행정구역','국가','확진자']) # 3개 변수에 대해 dropna\n",
    "\n",
    "df.fillna({'행정구역':'서울','국가':'대한민국'})\n",
    "df.fillna({'확진자':df['확진자'].mean()})\n",
    "df.fillna({'행정구역':'서울', '국가':'대한민국', '확진자':df['확진자'].mean()}, inplace=True) #inplace=True Df에 값을 저장\n",
    "df['Cabin'] = df['Cabin'].fillna('C000')\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "\n",
    "[drop]\n",
    "df = df.drop(columns=['A','C']) # A,C 컬럼 drop\n",
    "df = df.drop(['A','C'], axis = 1) # columns= 가 없으면 axis = 1로 컬럼 drop임을 지정\n",
    "df = df.drop(index=[1,4,5]) # 1,4,5 열 drop \n",
    "\n",
    "[drop_duplicates]\n",
    "df = df[['Line_No','Station_ID']].drop_duplicates()  # [[]] 이중 리스트 구조\n",
    "df = df.drop_duplicates(subset=['date','time','trans_id','item'])\n",
    "\n",
    "[reset_index]\n",
    "df = df.reset_index()\n",
    "\n",
    "[set_index] ## 시계열분석시 특정 col_1을 index로 지정\n",
    "df = df.set_index('col_1')\n",
    "\n",
    "[인덱싱과 필터링]\n",
    "df = df['sepal_length']    # []로 한개의 컬럼만 지정하면 Series객체를 반환\n",
    "df = df[['sepal_length']]  # [[]] 리스트로 컬럼을 지정하면 DataFrame으로 객체를 반환\n",
    "df = df[0:1] # 0행만 슬라이싱, 머신러닝시 Tip\n",
    "\n",
    "df = df.iloc[:3, 0:2]\n",
    "df = df.iloc[[1, 3, 5], [0, 3]] # 행, 컬럼 리스트 구조\n",
    "df = df.iloc[:,[1,2,7]] # 1,2,7번 컬럼만 필터\n",
    "\n",
    "df = df.loc[:,['A','B','C']] # A,B,C 컬럼만 필터\n",
    "df = df.loc[:2, 'petal_width'] # 한 개 컬럼일때 리스트 구조 없이 직접 입력\n",
    "df = df.loc[:2, ['sepal_length', 'petal_width']]  # 컬럼 리스트 구조\n",
    "df = df.loc[:2, 'petal_width':]\n",
    "df = df.loc[:2, :'petal_width']\n",
    "\n",
    "df = df.loc[df['BMI'] >  0, ] # 값중 > 0 를 필터\n",
    "df = df.loc[df['Segmentation'].isin(['A', 'D']), ].reset_index(drop = True) # 값중 A,D를 필터\n",
    "df = df.loc[df['Item'].isin(df_10['index']),] # 값중 df_10 'index' 컬럼내 값을 필터\n",
    "\n",
    "df = df.loc[(df['Pregnancies']== 0) & (df['BMI'] > 0),] # and 아니고 &, 두 조건 컬럼만 필터\n",
    "df = df.loc[(health['BP_HIGH'] > hi) | (health30['BP_HIGH'] < low), ] ## or 아니고 | 사용 주의\n",
    "df = df[df['condition'] != -1]\n",
    "df = df[df['Count']>50].sort_values('Mean', ascending = False) ## Hot\n",
    "df = df[df2['Admit']=='Admitted']\n",
    "\n",
    "df = df.query('Pregnancies== 0')  # query로 필터링\n",
    "\n",
    "[groupby]\n",
    "df = df.groupby(['cut','color'])[['price','carat']].mean().reset_index() ## 'price','carat' 컬럼의 mean값, [[ ]] 주의\n",
    "df = df.groupby(['cut','clarity'])['price'].mean().reset_index()\n",
    "df = df.groupby('Line_No')['Station_ID'].nunique().reset_index()\n",
    "\n",
    "[agg, aggregate] ## 집계함수 multi 적용\n",
    "df = df.groupby(['pet','color'])['breed_yn'].agg(['mean', 'min', 'max', 'count', 'size']).reset_index() #동일 컬럼 agg\n",
    "df = df.groupby(['pet']).agg({'Age':'max', 'SibSp':'sum', 'Fare':'mean'}).reset_index() # 다른 컬럼 다른 agg 지정\n",
    "df = df.groupby(['pet']).agg(Aeg_Cnt=('Age', 'count'), Age_Mean=('Age', 'mean'), Fare_Max=('Fare', 'max')) # 동일 컬럼 다른 agg 혼합\n",
    "\n",
    "[sort]\n",
    "df = df.sort_values( 'lift', axis=0, ascending=False) ## 'lift' 컬럼 기준 sort\n",
    "df = df.sort_values(['Station_ID','variable'], ascending=[False,True]) # 'Station_ID' 는 dscending,'variable'은 ascending \n",
    "\n",
    "[crosstab]\n",
    "df = pd.crosstab(dia['cut'],dia['clarity'], normalize = True).round(2)\n",
    "df = pd.crosstab(dia['cut'],dia['clarity'], normalize = 1).round(2)  # normalize = 1 컬럼기준으로 %\n",
    "df = pd.crosstab(dia['cut'],dia['clarity'], normalize = 0).round(2)  # normalize = 1 행기준으로 %\n",
    "df = pd.crosstab(dia['cut'],dia['clarity'],values = dia['price'],aggfunc = pd.Series.mean)\n",
    "\n",
    "[replace] # 변수내 값 변경\n",
    "df['Sex'] = df['Sex'].replace('male', 'Man') # Sex의 male값을 Man\n",
    "df['Sex'] = df['Sex'].replace({'male':'Man', 'female':'Woman'}) # Sex의 male값을 Man, female값을 Woman\n",
    "df['Cabin'] = df['Cabin'].replace(np.nan, 'C001')\n",
    "\n",
    "[변수내 value 값 조건에 따라 변경]\n",
    "tmp2 = np.where(tmp<7, tmp, 2*tmp) #np.where을 엑셀의 if문 처럼 사용\n",
    "\n",
    "[변수명인데 ' ' 없음]\n",
    "df_2014_2015 = df_mart_year_cnt_pivot.loc[(df_mart_year_cnt_pivot[2014] > 0) & (df_mart_year_cnt_pivot[2015] <= 0),] # '2014'시 error\n",
    "\n",
    "[Date 변수 처리]\n",
    "# 컬럼내 datetime 값을 YY, MM, DD 로 바꾸기\n",
    "# 컬럼내 datetime에서 원하는 값만 필터링 하기\n",
    "                                                                                           \n",
    "df['ymd']=df['date_column'].apply(pd.to_datetime)\n",
    "df['ymd']=pd.to_datetime(df['date_column'], format='%Y/%m/%d, %H:%M:%S')\n",
    "df['yy']=df.ymd.dt.year\n",
    "df['mm']=df['ymd'].dt.month # bike.ymd.dt vs bike['ymd'].dt 두 가지 포맷이 다 가능\n",
    "df['dd']=df.ymd.dt.day #날짜\n",
    "df['hour']=df.ymd.dt.hour\n",
    "df['wd']=df.ymd.dt.weekday #요일 숫자 (Monday=0, Sunday=6)\n",
    "df['wd']=df.ymd.dt.day_name() #요일 이름 메소드라 ()필요함 주의 \n",
    "df['wd']=df.ymd.dt.dt.month_name() #월 이름 메소드라 ()필요함 주의 \n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'], format='%m/%d/%Y, %H:%M:%S')\n",
    "df['D'] = (pd.to_datetime('31-08-2021',format = '%d-%m-%Y') - pd.to_datetime(df01['Dt_Customer'],format = '%d-%m-%Y')).dt.days # 날짜 수 계산시 추천\n",
    "df['stand_ym'] = df['date'].apply(lambda x: x[0:4]+x[5:7]) # 2001-03-04을 200103으로 만들기 lamda 함수 사용)\n",
    "df['birth_yr'] = df['jumin7'].apply(lambda x: '19' + x[0:2]).astype(int) # 주민번호로 년도 만들기\n",
    "df['Duration_Customer'] = df['Dt_Customer'].apply(lambda x:  (datetime.datetime.strptime('2021-08-31','%Y-%m-%d') - datetime.datetime.strptime(x,'%d-%m-%Y')).days)\n",
    "\n",
    "(datetime.datetime.strptime('2021-08-31','%Y-%m-%d')-datetime.datetime.strptime('2021-08-30','%Y-%m-%d')).days # row별로는 실행 가능\n",
    "* strptime은 문자열을 날짜로, strftime은 날짜를 문자열로 변환 주의:strptime은 Series 단위로 실행불가, row단위로 실행 <- lambda 사용\n",
    "\n",
    "[파생변수 생성]\n",
    "df['Sex_cd'] = (df['Sex'] == 'M') + 0    # 파생변수 값이 1,0일대 유용 \n",
    "df['ratio'] = df['price'] / df['carat']\n",
    "df['Age'] = (df['Age']//10) * 10  # 나이대 생성\n",
    "\n",
    "[파생변수 조건문 map 활용] # map dict 안에 없어 mapping되지 않는 값은 null값으로 변경됨 주의\n",
    "df['BP_cd'] = df['BP'].map({'LOW' : 0 , 'NORMAL' : 1 , 'HIGH' : 2})   # 3개중 선택, \n",
    "\n",
    "[변수명 변경]\n",
    "df = df.rename(columns = {'Sepal_Length': 'SL'}) # 기존 컬럼명 Sepal_Length'를 'SL' 로 바꾼다\n",
    "df.columns = ['SL', 'SW', 'PL', 'PW', 'Species'] # 기존 컬럼명을 'SL', 'SW', 'PL', 'PW', 'Species'로 바꾼다\n",
    "df.columns.values[0] = 'Sepal_Length' # 위 df.columns = ['SL', 'SW', 'PL', 'PW', 'Species']의 0번째 'SL'을 'Sepal_Length'로 변경\n",
    "\n",
    "[concat]\n",
    "pd.concat([bike_1,bike_2], axis = 1)\n",
    "df = pd.concat([df.loc[:,['Year_Month', 'Station_ID']],df.loc[:,'H01':'H24']], axis=1) # [ ] 주의\n",
    "\n",
    "[merge]\n",
    "pd.merge(left=df_A, right=df_B, how = 'inner', left_on='member', right_on='name')\n",
    "\n",
    "[melt]\n",
    "elec_melt=elec.melt(id_vars=['YEAR','MONTH','DAY'])\n",
    "\n",
    "[pivot]\n",
    "df = df.pivot_table(values='cnt', index='ID', \n",
    "                    columns='Item',  aggfunc='max', fill_value=0)\n",
    "                                                                    \n",
    "[shift] # 시계열, 브라이틱스 add lead lag, row방향 한칸 이동 \n",
    "df['mean_dow_1'] = df['mean_dow'].shift(1)    # mean_dow 열을 row방향으로 한 칸 이동\n",
    "\n",
    "[dummy]\n",
    "df = pd.get_dummies(data = df, columns = ['season']).drop_first = True # 기존 df에 더미변수 생성, but 기존 컬럼 삭제\n",
    "df_dum = pd.get_dummies(df['pet_category'], drop_first=True, prefix='pet_ca') # dummy 변수만 생성\n",
    "df = pd.concat([df, df_dum],axis=1) # 기존 df와 df_dum을 concat, 더미변수 원래 컬럼으로 groupby 해야할때 편리\n",
    "                                                                    \n",
    "[cut, 변수 등급화]\n",
    "pd.cut(arr, bins = [0,4,6,10], right=True, labels=['a','b','c']) # right=True면 right포함, False면 미포함\n",
    "df['Na_K_gr'] = pd.cut(df['Na_to_K'], bins = [0, 9, 19, 29, 39],\n",
    "       labels = ['Lv1', 'Lv2', 'Lv3','Lv4']) # labels 은 bins보다 1개 작아야 한다\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ab5b7",
   "metadata": {
    "id": "ca5ab5b7"
   },
   "source": [
    "# 검정과 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40412cef",
   "metadata": {
    "id": "40412cef"
   },
   "outputs": [],
   "source": [
    "[검정]\n",
    "from scipy.stats 에 있음\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "[t-test]\n",
    "from scipy.stats import ttest_1samp\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "[variance]\n",
    "from scipy.stats import bartlett\n",
    "from scipy.stats import levene\n",
    "from scipy.stats import f.cdf # F-test(cumulative distrubution function of F distribution)\n",
    "\n",
    "[ANOVA] f_oneway t검정과 같이 비교하는 데이터셋을 넣어준다 f_oneway(a,b,c,d) a는 Series, array등 데이터셋 \n",
    "from scipy.stats import f_oneway\n",
    "stat, p = f_oneway(df.loc[df['area'] == 1, 'house_price'],   #df.loc[df['area'] == 1  LOC 주의\n",
    "                   df.loc[df['area'] == 2, 'house_price'],\n",
    "                   df.loc[df['area'] == 3, 'house_price'])\n",
    "stat, p = f_oneway(df[df['area'] == 1]['house_price'],       #df[df['area'] == 1  LOC 없이 사용법\n",
    "                   df[df['area'] == 2]['house_price'],\n",
    "                   df[df['area'] == 3]['house_price'])\n",
    "\n",
    "[ANOVA] anova_lm    머신러닝 같이 모델 만들고 모델에 anova_lm 적용\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd # post-process of ANOVA \n",
    "model = ols(formula = 'Y ~ x1 + x2+ x1:x2', data = df).fit() # x1:x2는 x1:x2의 교호작용 볼때\n",
    "model = ols(formula = 'petal ~ C(x1)', data = df).fit(), # x1 범주 type이 int일때 str처리 필수 C(x1)\n",
    "anova_lm(model)\n",
    "model.fvalue # F값\n",
    "posthoc = pairwise_tukeyhsd(sns['use_time'], sns['age_grp'], alpha = 0.05)\n",
    "\n",
    "pairwise_tukeyhsd(endog = df3['item_amt'], groups = df3['age_cls']).summary()  해석 방법\n",
    "날짜계산\n",
    "\n",
    "[correlation]\n",
    "from scipy.stats import pearsonr # pearson\n",
    "from scipy.stats import spearmanr # spearman\n",
    "from scipy.stats import kendalltau # kendall\n",
    "\n",
    "df_corr.corr()\n",
    "pearsonr(ser1, ser2)\n",
    "## (0.8257047804016666, 0.04292079349918977)\n",
    "\n",
    "df[['casual','registered','count']].corr(method='spearman')\n",
    "df[['season','atemp','casual']].groupby('season').corr()\n",
    "pearsonr(df['casual'],df['registered'])\n",
    "\n",
    "[chi2]  corsstab 필수\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "cross = pd.crosstab(df['Gender'], df['Marital_status'])\n",
    "chi2_contingency(cross)\n",
    "chi2_contingency(cross, correction=False)  ## 문제지 가이드에 correction=False 나올때 주의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55eccf0",
   "metadata": {
    "id": "c55eccf0"
   },
   "source": [
    "# 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f703f",
   "metadata": {
    "id": "672f703f"
   },
   "source": [
    "* 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a75f3",
   "metadata": {
    "id": "f34a75f3"
   },
   "outputs": [],
   "source": [
    "[normalization]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "[MinMax 정규화]   ## Train 데이터를 정규화 하고 그 모델을 Test 데이터에 적용\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "AAmodel = MinMaxScaler().fit(X = df_train[['x1', 'x2']]) # MinMaxScaler로 'x1' 'x2' 변수를 모델링후 fit까지\n",
    "AAmodel_train = AAmodel.transform(X = df_train[['x1', 'x2']])  # AAmodel_train에 MINMAX 값 저장\n",
    "df_train[['mx1','mx2']] = AAmodel_train # AAmodel_train 값으로 파생변수 'mx1' 'mx2' 생성\n",
    "\n",
    "AAmodel_test = AAmode.transform(X = df_test[['x1', 'x2']]) # Train에서 생성된 AAmodel로 test에 MINMAX 값 생성\n",
    "df_test[['mx','mc']] = AAmodel_tset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd72733",
   "metadata": {
    "id": "2bd72733"
   },
   "source": [
    "* 머신러닝 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ad0ba",
   "metadata": {
    "id": "467ad0ba"
   },
   "outputs": [],
   "source": [
    "[Statsmodels]\n",
    "Statsmodels 란 다양한 통계 모델을 평가하고 통계 테스트를 수행하고 통계 데이터를 탐색하는 데 필요한 \n",
    "클래스와 함수를 제공하는 Python모듈이다. \n",
    "우리가 사용할 최소제곱법을 활용한 선형회귀분석도 포함되어있다.\n",
    "\n",
    "\n",
    "다중회귀 독립변수 입력\n",
    "'season ~ ' + ' + '.join(df_sub.columns[:-1])\n",
    "'season ~ ' + ' + '.join(['col_A', 'col_B', 'col_C']) ## 'y ~ col_A + col_B + col_C'\n",
    "\n",
    "독립변수 표기 2가지\n",
    "X = df.loc[:, ['Pregnancies','Glucose','BloodPressure', 'Insulin', 'BMI']]\n",
    "X = df.[['Pregnancies','Glucose','BloodPressure', 'Insulin', 'BMI']]  # 2차원 DataFrame 형태로 만들면 loc와 같다\n",
    "y = df['종속변수']\n",
    "\n",
    "\n",
    "from statsmodels.formula.api import ols   # statsmodels 통계 중심\n",
    "from sklearn.cluster import AgglomerativeClustering  # sklearn 머신러닝 중심\n",
    "from sklearn.metrics import accuracy_score # 평가는 metrics\n",
    "\n",
    "\n",
    "데이터 분할\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, train_size = 0.7, random_state=123)\n",
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "\n",
    "\n",
    "모델링 - 단순선형회귀\n",
    "선형회귀는 statsmodels 의 ols() 함수를 사용한 방법과 sklearn 의 LinearRegression() 함수를 사용한 방법이 있다. \n",
    "statsmodels 는 각종 통계량과 함께 상세한 선형회귀 결과를 확인할 수 있다.\n",
    "\n",
    "[statsmodels]\n",
    "from statsmodels.formula.api import ols\n",
    "model = ols(data = df, formula = 'Y ~ x1 + x2 + x3' ).fit()\n",
    "model.summary()\n",
    "model.rsquared\n",
    "pred_s = model_s.predict(df_s_test)\n",
    "pred_s[:4]\n",
    "\n",
    "[sklearn]\n",
    "sklearn 은 .summary() 메서드가 없고 상세 내역을 arribute를 사용하여 뽑아야 한다.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept = True, normalize = True)\n",
    "model.fit(X = df[['SepalWidth', 'PetalLength']], y = df['SepalLength']) # X = df[[ ]], X는 겹 list 구조 필수 주의 \n",
    "model.coef_\n",
    "## array([0.59552475, 0.47192004])\n",
    "\n",
    "model_1.intercept_\n",
    "## 2.249140160383228\n",
    "\n",
    "모델링 - 다중선형회귀, VIF 계산\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "from patsy import dmatrices\n",
    "formula = 'season ~ ' + ' + '.join(df_sub.columns[:-1])\n",
    "y, X = dmatrices(formula, data = df_sub, return_type='dataframe')  #df_sub의 컬럼중 y, X를 구분하여 y, X 데이터프레임 생성\n",
    "df_vif = pd.DataFrame() #빈 df_vif 데이터프레임 생성 \n",
    "df_vif['colname']=X.columns\n",
    "df_vif['VIF']=[vif(X.values, i) for i in range(X.shape[1])] \n",
    "df_vif # VIF값이 10보다 큰 것을 골라낸다\n",
    "\n",
    "모델링 - 다중선형회귀\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import ols\n",
    "model = ols(formula = 'price ~ carat + depth', data = df_dia).fit()\n",
    "model.summary()\n",
    "pred = model.predict(df_dia)\n",
    "\n",
    "모델링 - 계층적 군집분석\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "model=AgglomerativeClustering(n_clusters=3).fit(df_sub)\n",
    "model.labels_\n",
    "df['cluster']=model.labels_ # df에 cluster컬럼 생성\n",
    "pd.crosstab(df['Species'],df['cluster']) #원하는 컬럼과 Cluster crosstab\n",
    "df.groupby('cluster').mean().reset_index()\n",
    "link = linkage(df_sub, method='ward')\n",
    "plt.figure()\n",
    "dendrogram(link)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "모델링 - 로지스틱회귀\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.api import Logit\n",
    "model = Logit(endog = df['is_setosa'], \n",
    "              exog = df.iloc[:, :2]).fit()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=123)\n",
    "model.fit(X = df.iloc[:, :2],\n",
    "         y = df['is_setosa'])\n",
    "pred = model.predict_proba(X = df.iloc[:, :2])\n",
    "accuracy_score(y_true = df['is_setosa'],\n",
    "               y_pred = (pred1 > 0.5) + 0)\n",
    "model.params # Odd Ratio\n",
    "np.exp(model.params) # Odd Ratio\n",
    "\n",
    "\n",
    "모델링 - 나이브베이즈\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB().fit(X = df.iloc[:,:4], y = df['is_setosa'])\n",
    "model.class_prior_  # 사전확률\n",
    "\n",
    "model = MultinomialNB(fit_prior=False).fit(X=x3tr_col, y = y3tr_col)  # fit_prior=False \n",
    "\n",
    "모델링 - KNN\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X, y) \n",
    "pred = model.predict(X)\n",
    "\n",
    "모델링 - K-Means\n",
    "K-means Silhouette 를 통해 구한 최적의 K로 설정한다.\n",
    "smodel_ft = smodel.fit_transform(X = df_02[:, ['BALANCE' : 'TENURE']]) 'BALANCE' ~ 'TENURE'  \n",
    "\n",
    "\n",
    "모델링 - 의사결정나무\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree\n",
    "model = DecisionTreeClassifier(random_state=123)\n",
    "model = DecisionTreeRegressor(random_state=123)\n",
    "model.fit(X, y) \n",
    "pred = model.predict(X)\n",
    "\n",
    "연관성 분석\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "df['cnt'] = 1  # pivot_table value값을 사전 생성\n",
    "df_dup0 = df.iloc[:, 1:].drop_duplicates()  # 중복제거, Unique\n",
    "df_pivot = df_dup0.pivot_table(values='cnt', index='ID',\n",
    "                                       columns='Item',  aggfunc='max', fill_value=0)\n",
    "df_apr = apriori(df_pivot, min_support=0.005, use_colnames=True, max_len=3)\n",
    "df_asso = association_rules(df_apr, metric='confidence',  min_threshold=0.005)\n",
    "df_asso1 = df_asso.loc[df_asso['support'] >= 0.01, ]\n",
    "df_asso1 = df_asso1.sort_values('lift', ascending=False)\n",
    "\n",
    "df = asso3[(asso3['antecedents'].apply(lambda x: len(x)) == 1)] \n",
    "      & (asso3[asso3['consequents'].apply(lambda x: len(x)) == 1)]  # 'antecedents' 'consequents'이 단일 아이템인것 추출\n",
    "\n",
    "df_asso['antecedents'].values로 보면 frozenset({'Toast'}) 보임\n",
    "df_asso[df_asso['antecedents'] == frozenset({'Toast'})] # 추출시 'frozenset({'Toast'})'가 아니고 frozenset({'Toast'}) 주의 \n",
    "\n",
    "PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(random_state=1234, svd_solver='full')\n",
    "pca.fit(X_scaled)\n",
    "pca.explained_variance_ratio_\n",
    "np.cumsum(pca.explained_variance_ratio_)  \n",
    "## [0.76549057, 0.88480564, 0.93822969, 0.97057075, 1.] -> select 2 components\n",
    "\n",
    "pca_component = pd.DataFrame(pca.fit_transform(X_scaled))[[0, 1]].add_prefix('pca_')\n",
    "df2_train = pd.concat([df2, pca_component], axis=1)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier # k-NN\n",
    "from sklearn.neighbors import KNeighborsRegressor # k-NN\n",
    "from sklearn.cluster import AgglomerativeClustering # 계층적 준집 분석\n",
    "from sklearn.cluster import KMeans # K-means\n",
    "from statsmodels.formula.api import ols # 선형회귀\n",
    "from sklearn.linear_model import LinearRegression  # 선형회귀\n",
    "from statsmodels.api import Logit # 로지스틱 회귀\n",
    "from sklearn.linear_model import LogisticRegression # 로지스틱 회귀\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree\n",
    "from sklearn.naive_bayes import GaussianNB # Naive Bayes\n",
    "\n",
    "\n",
    "평가 - 회귀\n",
    "from sklearn.metrics import mean_absolute_error # MAE\n",
    "from sklearn.metrics import mean_absolute_percentage_error # MAPE\n",
    "from sklearn.metrics import mean_squared_error # MSE\n",
    "\n",
    "mean_squared_error(y_true = df_test['BMI'], y_pred = pred_r)**0.5 # RMSE\n",
    "\n",
    "평가 - 분류\n",
    "from sklearn.metrics import roc_auc_score # AUC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_reporty_pred = pred_dia, y_true = df_test['Outcome']) # print 안쓰면 읽기 어려움\n",
    "accuracy_score(y_pred = pred_dia, y_true = df_test['Outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e2bc8",
   "metadata": {
    "id": "e62e2bc8"
   },
   "source": [
    "* class 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43afa3",
   "metadata": {
    "id": "6a43afa3"
   },
   "outputs": [],
   "source": [
    "import\tdatetime\n",
    "import\tmatplotlib.pyplot\tas\tplt\t\n",
    "import\tos\t\t\t\n",
    "from\tmatplotlib\timport\tpyplot as plt\t\n",
    "from\tmlxtend.frequent_patterns\timport\tapriori\t\n",
    "from\tmlxtend.frequent_patterns\timport\tassociation_rules\t\n",
    "from\tpatsy\timport\tdmatrices\t\n",
    "from\tscipy.cluster.hierarchy\timport\tdendrogram, linkage\t\n",
    "from\tscipy.stats\timport\tbartlett\t\n",
    "from\tscipy.stats\timport\tchi2_contingency\t #Chi\n",
    "from\tscipy.stats\timport\tf.cdf\t#F-test(cumulativedistrubutionfunctionof\n",
    "from\tscipy.stats\timport\tf_oneway\t #ANOVA\n",
    "from\tscipy.stats\timport\tkendalltau\t#kendall\n",
    "from\tscipy.stats\timport\tlevene\t\n",
    "from\tscipy.stats\timport\tpearsonr\t#pearson\n",
    "from\tscipy.stats\timport\tspearmanr\t#spearman\n",
    "from\tscipy.stats\timport\tttest_1samp\t\n",
    "from\tscipy.stats\timport\tttest_ind\t\n",
    "from\tscipy.stats\timport\tttest_rel\t\n",
    "from\tsklearn.cluster\timport\tAgglomerativeClustering\t#계층적군집분석\n",
    "from\tsklearn.cluster\timport\tKMeans\t#K-means\n",
    "from\tsklearn.linear_model\timport\tLinearRegression\t#선형회귀\n",
    "from\tsklearn.linear_model\timport\tLogisticRegression\t#로지스틱회귀\n",
    "from\tsklearn.metrics\timport\taccuracy_score\t#평가는metrics\n",
    "from\tsklearn.metrics\timport\tf1_score\t\n",
    "from\tsklearn.metrics\timport\tmean_absolute_error\t#MAE\n",
    "from\tsklearn.metrics\timport\tmean_absolute_percentage_error\t#MAPE\n",
    "from\tsklearn.metrics\timport\tmean_squared_error\t#MSE\n",
    "from\tsklearn.metrics\timport\tprecision_score\t\n",
    "from\tsklearn.metrics\timport\trecall_score\t\n",
    "from\tsklearn.metrics\timport\troc_auc_score\t#AUC\n",
    "from  sklearn.metrics import  classification_report # confusion matrix summary report\n",
    "from\tsklearn.model_selection\timport\ttrain_test_split\t\n",
    "from\tsklearn.naive_bayes\timport\tGaussianNB\t#NaiveBayes 연속형 변수\n",
    "from\tsklearn.naive_bayes\timport\tMultinomialNB\t#NaiveBayes 빈도수\n",
    "from\tsklearn.naive_bayes\timport\tCategoricalNB\t#NaiveBayes  범주형 변수\n",
    "from\tsklearn.neighbors\timport\tKNeighborsClassifier\t#k-NN\n",
    "from\tsklearn.neighbors\timport\tKNeighborsRegressor\t#k-NN\n",
    "from\tsklearn.preprocessing\timport\tMinMaxScaler\t\n",
    "from\tsklearn.preprocessing\timport\tStandardScaler\t\n",
    "from\tsklearn.tree\timport\tDecisionTreeClassifier\t#DecisionTree\n",
    "from\tsklearn.tree\timport\tDecisionTreeRegressor\t#DecisionTree\n",
    "from\tstatsmodels.api\timport\tLogit\t#로지스틱회귀\n",
    "from\tstatsmodels.formula.api\timport\tols\t\n",
    "from\tstatsmodels.stats.anova\timport\tanova_lm\t\n",
    "from\tstatsmodels.stats.multicomp\timport\tpairwise_tukeyhsd\t#post-processofANOVA\n",
    "from\tstatsmodels.stats.outliers_influence\timport\tvariance_inflation_factor as VIF\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6e4a8",
   "metadata": {
    "id": "a3a6e4a8"
   },
   "source": [
    "* 날짜 포맷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596c2c9",
   "metadata": {
    "id": "6596c2c9"
   },
   "outputs": [],
   "source": [
    "Directive\tMeaning\tExample\n",
    "%a\t짧은 요일이름\tSun, Mon, …, Sat..\n",
    "%A\t전체 요일이름\tSunday, Monday, …, Saturday\n",
    "%w\t숫자로 표현한 요일, 0이 월요일, 6이 일요일임\t0, 1, …, 6\n",
    "%d\t연월일시의 '일'자. 0 패딩 들어감\t01, 02, 03, ..., 31\n",
    "%b\t짧은 월 이름\tJan, Feb, ...\n",
    "%B\t전체 월 이름\tJanuary, February, …, December\n",
    "%m\t연월일시의 '월', 0 패딩 들어감\t01, 02, ..., 12\n",
    "%y\t연도 뒤의 2자리 (2021년의 경우 21)\t00, 01, ..., 99\n",
    "%Y\t연도 4자리, 0 패딩 들어감\t0001, 0002, ..., 2013, 2014, ..., 9999\n",
    "%H\t24시간제의 시간, 0 패딩 들어감, 24가 없고 00이 있음\t00, 01, 02, ..., 23\n",
    "%p\tAM or PM\tAM, PM\n",
    "%M\t시간: 분: 초의 분, 0 패딩 들어감\t00, 01, ..., 59\n",
    "%S\t시간: 분: 초의 초, 0 패딩 들어감\t00, 01, ..., 59\n",
    "%j\t1년 중 몇 번째 '일'인지, 0 패딩 들어감\t001, 002, ..., 366\n",
    "%U\t1년 중 몇 번째 '주'인지(일요일 시작 기준), 0 패딩 들어감\t00, 01, ..., 53\n",
    "%W\t1년 중 몇 번째 '주'인지(월요일 시작 기준), 0 패딩 들어감\t00, 01, ..., 53"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
